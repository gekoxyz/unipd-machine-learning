{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fundamentals of Information Systems\n",
    "\n",
    "## Python Programming (for Data Science)\n",
    "\n",
    "### Master's Degree in Data Science\n",
    "\n",
    "#### Giorgio Maria Di Nunzio\n",
    "#### (Courtesy of Gabriele Tolomei FIS 2018-2019)\n",
    "<a href=\"mailto:giorgiomaria.dinunzio@unipd.it\">giorgiomaria.dinunzio@unipd.it</a><br/>\n",
    "University of Padua, Italy<br/>\n",
    "2021/2021<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 12: The Regression Problem - Example (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Instructions\n",
    "\n",
    "-  We consider the dataset available at this [link](http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data), provided by the [UC Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml/index.php)\n",
    "\n",
    "-  The dataset comes with a [**README**](http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.names) file, which contains information about the number of instances, the number and type of attributes, as well as the prediction goal.\n",
    "\n",
    "-  To be able to work even without any network connection, I have stored both the dataset and README files locally on my machine.\n",
    "\n",
    "-  \"The data concerns city-cycle fuel consumption in miles per gallon, to be predicted in terms of 3 multivalued discrete and 5 continuous attributes.\" (Quinlan, 1993)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Additional Notes\n",
    "\n",
    "-  The original file contains a mixture of whitespace and tab-separated fields. In order to transform it into a legitimate tab-separated file, I had to run the following shell commands:\n",
    "\n",
    "```bash\n",
    "> TAB=$(printf '\\t')\n",
    "> sed \"s/ \\{2,\\}/$TAB/g\" < ${ORIGINAL_DATASET_FILE} > ${NEW_DATASET_FILE}\n",
    "```\n",
    "\n",
    "-  The commands above use <code>**sed**</code> to replace **2 or more** whitespaces with a tab character on **every line** of the original dataset file.\n",
    "\n",
    "-  Linux and Mac OS X systems have <code>**sed**</code> natively installed. Windows users can install it from [here](http://gnuwin32.sourceforge.net/packages/sed.htm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Import stats module from scipy, which contains a large number \n",
    "# of probability distributions as well as an exhaustive library of statistical functions.\n",
    "import scipy.stats as stats\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Path to the local dataset file\n",
    "DATASET_PATH = \"./data/auto-mpg-regression/dataset.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset with Pandas\n",
    "data = pd.read_csv(DATASET_PATH, sep=\"\\t\")\n",
    "\n",
    "print(\"Shape of the dataset: {}\".format(data.shape))\n",
    "data.head()\n",
    "# NOTE: the first line of the file is considered as the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset with Pandas, this time taking into account\n",
    "# the fact that there is no header line\n",
    "data = pd.read_csv(DATASET_PATH, sep=\"\\t\", header=None)\n",
    "print(\"Shape of the dataset: {}\".format(data.shape))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Column names (labels) are not so meaningful and we should use the attribute names\n",
    "# provided in the README file\n",
    "# Row index, instead, can be left as it is (i.e., default IndexRange)\n",
    "columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', \n",
    "           'acceleration', 'model_year', 'origin', 'car_name']\n",
    "\n",
    "data = pd.read_csv(DATASET_PATH, sep=\"\\t\", \n",
    "                   header=None, \n",
    "                   names=columns)\n",
    "\n",
    "print(\"Shape of the dataset: {}\".format(data.shape))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Checking for any Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Check if there is any missing value in the whole dataset\n",
    "print(\"There are missing values in the dataset: {}\".\n",
    "     format(data.isnull().any().any()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Check if 'horsepower' has really 6 missing values\n",
    "print(\"N. of missing values for attribute 'horsepower': {}\".\n",
    "     format(data.horsepower.isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.horsepower.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Weird... we know from the README file there must be 6 missing values\n",
    "# How come we are not able to spot those?\n",
    "# Possibly, because of the way in which NAs are 'encoded' in the file...\n",
    "# Let's see the set of values contained in the 'horsepower' column\n",
    "print(sorted(data.horsepower.unique(), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Apparently, there are some question mark characters '?' in this column.\n",
    "# Let's see how many records have '?' in their 'horsepower' column.\n",
    "# Extract the sub-DataFrame using boolean indexing \n",
    "# on the 'horsepower' column and count the corresponding number of matching rows.\n",
    "print(\"How many records have 'horsepower=?'?: {}\"\n",
    "      .format(data[data.horsepower == '?'].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.horsepower == '?'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.horsepower == \"?\", :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We have therefore found that '?' is a sentinel value used to identify NAs\n",
    "# Let's reload the dataset using this value as a marker for NAs.\n",
    "data = pd.read_csv(DATASET_PATH, sep=\"\\t\", header=None, \n",
    "                   names=columns,\n",
    "                  na_values={'horsepower':'?'})\n",
    "\n",
    "# Alternatively, we could simply replace '?' on the loaded dataset with np.nan\n",
    "# 1. Using 'loc':\n",
    "# data.loc[data.horsepower == '?', 'horsepower'] = np.nan\n",
    "# 2. Using 'replace':\n",
    "# data.horsepower.replace('?', np.nan, inplace=True)\n",
    "print(\"Shape of the dataset: {}\".format(data.shape))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's repeat the same check as above on missing values.\n",
    "# Check if there is any missing value in the whole dataset\n",
    "print(\"There are missing values in the dataset: {}\".\n",
    "     format(data.isnull().any().any()))\n",
    "# Check if 'horsepower' has really 6 missing values\n",
    "print(\"N. of missing values for attribute 'horsepower': {}\".\n",
    "     format(data.horsepower.isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's have a look at the output of the 'describe()' function.\n",
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"mpg\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Change the Column Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Just as a convention, I prefer to place the column to be predicted\n",
    "# as the last one.\n",
    "columns = data.columns.tolist()\n",
    "print(\"Orignal order of columns:\\n{}\".format(columns))\n",
    "\n",
    "# Popping out 'mpg' from the list and insert it back at the end.\n",
    "columns.insert(len(columns), columns.pop(columns.index('mpg')))\n",
    "\n",
    "print(\"New order of columns:\\n{}\".format(columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's refactor the DataFrame using this new column index\n",
    "data = data.loc[:, columns]\n",
    "data.head()\n",
    "# Alternatively to 'loc' we can also use 'reindex()'\n",
    "# data = data.reindex(columns=columns)\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Apply Some Simple Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Suppose we want to convert displacement unit from cubic inch to litre\n",
    "# There is a useful conversion table which tells us how to do that.\n",
    "# 1 cubic inch = 0.016387064 litre\n",
    "CI_TO_LITRE = 0.016387064\n",
    "data.displacement *= CI_TO_LITRE\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.1 Analysis of Data Distributions: Continous Values\n",
    "\n",
    "-  Let's start visualizing the distributions of the **5 continuous-valued** features:\n",
    "    - <code>**displacement**</code>\n",
    "    - <code>**horsepower**</code> (contains 6 <code>**NA**</code>s)\n",
    "    - <code>**weight**</code>\n",
    "    - <code>**acceleration**</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a lambda function which will be applied to each entry\n",
    "# of the numpy 2-D array of AxesSubplot objects\n",
    "# x is a reference to an AxesSubplot object\n",
    "y_labeler = lambda x: x.set_ylabel('density')\n",
    "# np.vectorize() allows calling the function on each element\n",
    "y_labeler = np.vectorize(y_labeler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a Figure containing 2x2 subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,12))\n",
    "# Call the vectorized function for labeling all the y-axes\n",
    "y_labeler(axes)\n",
    "\n",
    "# Plot 'displacement' on the top-left subplot\n",
    "sns.distplot(data.displacement, \n",
    "             color='#808080', \n",
    "             ax=axes[0,0], \n",
    "             hist_kws=dict(edgecolor=\"#404040\", linewidth=1))\n",
    "\n",
    "# Plot 'horsepower' (limited only to non-NA values) on the top-right subplot\n",
    "sns.distplot(data.loc[data.horsepower.notnull(), 'horsepower'], \n",
    "                 color='#df2020', ax=axes[0,1], \n",
    "                 hist_kws=dict(edgecolor=\"#404040\", linewidth=1))\n",
    "\n",
    "# Plot 'weight' on the bottom-left subplot\n",
    "sns.distplot(data.weight, color='#0033cc', ax=axes[1,0], \n",
    "                 hist_kws=dict(edgecolor=\"k\", linewidth=1))\n",
    "\n",
    "# Plot 'acceleration' on the bottom-right subplot\n",
    "sns.distplot(data.acceleration, color='#009933', ax=axes[1,1],\n",
    "                 hist_kws=dict(edgecolor=\"#006622\", linewidth=1))\n",
    "\n",
    "# Adjust space between plots\n",
    "plt.subplots_adjust(wspace=.3, hspace=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Additional Note on Binning\n",
    "\n",
    "-  If we don't specify the number of **bins** as argument of <code>**sns.distplot**</code> function (i.e.,<code>**bins=None**</code>) the **Freedman-Diaconis** rule is used to devise the _best_ number of bins.\n",
    "\n",
    "-  This rule starts from defining the **width** each bin should have on the basis of the range of values observed, as follows:\n",
    "\n",
    "$$\n",
    "\\texttt{bin}\\_\\texttt{width} = 2 * \\frac{\\texttt{IQR}}{n^{1/3}}\n",
    "$$\n",
    "where $\\texttt{IQR}$ stands for **interquartile range**, namely the length of the interval delimited by the 1st and the 3rd quartile, and $n$ is the **number of observations**.\n",
    "\n",
    "-  Finally, the number of bins is computed as:\n",
    "\n",
    "$$\n",
    "\\texttt{bins} = (maxâˆ’min)/\\texttt{bin}\\_\\texttt{width}\n",
    "$$\n",
    "\n",
    "where, $max$ ($min$) is the **maximum** (**minimum**) value observed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's produce the boxplots corresponding to the distribution plots above\n",
    "# Create a Figure containing 2x2 subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,12))\n",
    "\n",
    "sns.boxplot(data.displacement, color='#808080', ax=axes[0,0])\n",
    "sns.boxplot(data.loc[data.horsepower.notnull(), 'horsepower'], \n",
    "                color='#df2020', ax=axes[0,1])\n",
    "sns.boxplot(data.weight, color='#0033cc', ax=axes[1,0])\n",
    "sns.boxplot(data.acceleration, color='#009933', ax=axes[1,1])\n",
    "plt.subplots_adjust(wspace=.3, hspace=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spotting Outliers\n",
    "\n",
    "-  Generally speaking, an outlier is an observation that is numerically distant from the rest of the data. \n",
    "\n",
    "-  Boxplots are useful to actually spot any possible **outlier**, as they show the distribution of values that are located within the $\\texttt{IQR}$ (i.e., any data point between the 1st and 3rd quartile).\n",
    "\n",
    "-  A boxplot defines also 2 other values, called **fences** or **whiskers** which are used to define outliers (i.e., any data point that is located outside the fences).\n",
    "\n",
    "-  Usually, fences are determined as follows: if $Q_1$ and $Q_3$ represents the 1st and 3rd quartile, respectively, we define $F_\\textrm{left}$ and $F_\\textrm{right}$ as the left and right fence point, respectively, so that:\n",
    "\n",
    "$$\n",
    "F_\\textrm{left} = Q_1 - 1.5 * \\texttt{IQR};~~F_\\textrm{right} = Q_3 + 1.5 * \\texttt{IQR}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Few Observations from the Plots\n",
    "\n",
    "-  <code>**displacement**</code> distribution is not uni-modal at all; in fact it is bi-, or possibly, tri-modal. Apparently, this is not affected by any outlier.\n",
    "\n",
    "-  <code>**horsepower**</code> distribution is also not uni-modal, with a small bump around the value of 150Hp. However, here we can spot some possible outliers (i.e., large values over 200Hp).\n",
    "\n",
    "-  <code>**weight**</code> distribution is uni-modal yet **right-skewed** (**positively skewed**), which means the mean is shifted to the right due to the presence of some large values towards the positive direction (not necessarily outliers).\n",
    "\n",
    "-  <code>**acceleration**</code> essentially fits nicely to a **Normal** (**Gaussian**) **distribution** and contains some outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Check how many outliers we have\n",
    "# 1. 'horsepower'\n",
    "hp_q1, hp_q3 = data.loc[data.horsepower.notnull(), 'horsepower'].quantile([.25, .75])\n",
    "print(\"1st Quartile of 'horsepower': {:.2f}\".\n",
    "      format(hp_q1))\n",
    "print(\"3rd Quartile of 'horsepower': {:.2f}\".\n",
    "      format(hp_q3))\n",
    "hp_IQR = (hp_q3 - hp_q1)\n",
    "print(\"IQR of 'horsepower': {:.2f}\".\n",
    "      format(hp_IQR))\n",
    "hp_fence_left = hp_q1 - 1.5 * hp_IQR\n",
    "hp_fence_right = hp_q3 + 1.5 * hp_IQR\n",
    "print(\"Fence range: [{:.2f}, {:.2f}]\".\n",
    "      format(hp_fence_left, hp_fence_right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"N. of instances containing outlier of 'horsepower': {}\".\n",
    "      format(data[data.horsepower > hp_fence_right].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.horsepower > hp_fence_right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Check how many outliers we have\n",
    "# 2. 'acceleration'\n",
    "acc_q1, acc_q3 = data['acceleration'].quantile([.25, .75])\n",
    "print(\"1st Quartile of 'acceleration': {:.2f}\".\n",
    "      format(acc_q1))\n",
    "print(\"3rd Quartile of 'acceleration': {:.2f}\".\n",
    "      format(acc_q3))\n",
    "acc_IQR = (acc_q3 - acc_q1)\n",
    "print(\"IQR of 'acceleration': {:.2f}\".\n",
    "      format(hp_IQR))\n",
    "acc_fence_left = acc_q1 - 1.5 * acc_IQR\n",
    "acc_fence_right = acc_q3 + 1.5 * acc_IQR\n",
    "print(\"Fence range: [{:.2f}, {:.2f}]\".\n",
    "      format(acc_fence_left, acc_fence_right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"N. of instances containing outlier of 'horsepower' or 'acceleration': {}\".\n",
    "      format(data[(data.acceleration < acc_fence_left) | \n",
    "                  (data.acceleration > acc_fence_right)].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"N. of instances containing outlier of 'acceleration': {}\".\n",
    "      format(data[(data.horsepower > hp_fence_right) |\n",
    "                  (data.acceleration > acc_fence_right) |\n",
    "                  (data.acceleration < acc_fence_left)].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's see if we can spot where those outliers are located\n",
    "# w.r.t. other features (e.g., cylinders)\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10,8))\n",
    "\n",
    "sns.boxplot(x=data.cylinders, y=data.loc[data.horsepower.notnull(), 'horsepower'], \n",
    "                color='#df2020', ax=axes[0])\n",
    "sns.boxplot(x=data.cylinders, y=data.acceleration, color='#009933', ax=axes[1])\n",
    "plt.subplots_adjust(wspace=.3, hspace=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other Few Observations from the Plots\n",
    "\n",
    "-  <code>**horsepower**</code>: outliers seem to occur only on 6- and 8-cylinder vehicles, stronger in the former.\n",
    "\n",
    "-  <code>**acceleration**</code>: outliers here are more evenly distributed over 3- 4- and 8-cylinder vehicles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's now plot the pairwise relationship between our continuous-valued features\n",
    "sns.pairplot(data.loc[data.horsepower.notnull(),\n",
    "                          ['displacement', 'horsepower', 'weight', \n",
    "                           'acceleration']],\n",
    "                 kind=\"reg\",\n",
    "                 diag_kind='kde', \n",
    "                 diag_kws={'shade': True, 'color': '#ff6600'}, \n",
    "                 plot_kws={'color': '#ff6600'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Continuous-valued Features Relate to Each Other\n",
    "\n",
    "-  As <code>**displacement**</code> increases, so do <code>**weight**</code> and <code>**horsepower**</code>; also <code>**acceleration**</code> tends to decrease (pretty intuitive!).\n",
    "\n",
    "-  As <code>**horsepower**</code> increases, <code>**acceleration**</code> decreases, whilst <code>**weight**</code> and <code>**displacement**</code> increase as well.\n",
    "\n",
    "-  As <code>**weight**</code> increases, both <code>**horsepower**</code> and <code>**displacement**</code> increases but <code>**acceleration**</code> tends to decrease. Now, this might seem counterintuitive at first but the thing is that there might be a **latent factor** which affects this relationship (e.g., a heavy vehicle is also likely to have more horsepower and therefore this could be the actual reason why we observe such a phenomenon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's now plot the pairwise relationship between our continuous-valued features \n",
    "# this time also considering our target variable 'mpg'\n",
    "sns.pairplot(data.loc[data.horsepower.notnull(),\n",
    "                          ['displacement', 'horsepower', 'weight', 'acceleration', 'mpg']],\n",
    "                 kind=\"reg\",\n",
    "                 diag_kind='kde', \n",
    "                 diag_kws={'shade': True, 'color': '#33cccc'}, \n",
    "                 plot_kws={'color': '#33cccc'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.1 Analysis of Data Distributions: Categorical Values\n",
    "\n",
    "-  Let's visualize the distributions of the **3 categorical** features:\n",
    "    - <code>**cylinders**</code>\n",
    "    - <code>**model_year**</code>\n",
    "    - <code>**origin**</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's see the frequency counts of each categorical variable\n",
    "# 'cylinders'\n",
    "print(data.cylinders.value_counts())\n",
    "print()\n",
    "# 'model_year'\n",
    "print(data.model_year.value_counts())\n",
    "print()\n",
    "# 'origin'\n",
    "print(data.origin.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's produce the boxplots corresponding to the distribution plots above\n",
    "# Create a Figure containing 2x2 subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(8,4))\n",
    "y_labeler(axes)\n",
    "\n",
    "# Plot 'cylinders'\n",
    "sns.distplot(data.cylinders, color='#006699', ax=axes[0], \n",
    "                 kde=False, norm_hist=True,\n",
    "                 hist_kws=dict(edgecolor=\"#404040\", linewidth=1))\n",
    "# Plot 'model_year'\n",
    "sns.distplot(data.model_year, color='#a6cc33', ax=axes[1], \n",
    "                 kde=False, norm_hist=True,\n",
    "                 hist_kws=dict(edgecolor=\"#85a329\", linewidth=1))\n",
    "# Plot 'origin'\n",
    "sns.distplot(data.origin, color='#cc3399', ax=axes[2], \n",
    "                 kde=False, norm_hist=True,\n",
    "                 hist_kws=dict(edgecolor=\"#8f246b\", linewidth=1))\n",
    "# Adjust space between plots\n",
    "plt.subplots_adjust(wspace=.5, hspace=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# For categorical variables, 'countplot' is the way to go\n",
    "# Create a Figure containing 1x3 subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12,4))\n",
    "\n",
    "# Plot 'cylinders'\n",
    "sns.countplot(data.cylinders, ax=axes[0])\n",
    "sns.countplot(data.model_year, ax=axes[1])\n",
    "sns.countplot(data.origin, ax=axes[2])\n",
    "plt.subplots_adjust(wspace=.5, hspace=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# stripplot is also another useful plot to relate categorical vs. target variable\n",
    "# Create a Figure containing 1x3 subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12,4))\n",
    "\n",
    "sns.stripplot(x=data.cylinders, y=data.mpg, color='#006699', ax=axes[0])\n",
    "sns.stripplot(x=data.model_year, y=data.mpg, color='#a6cc33', ax=axes[1])\n",
    "sns.stripplot(x=data.origin, y=data.mpg, color='#cc3399', ax=axes[2])\n",
    "\n",
    "plt.subplots_adjust(wspace=.5, hspace=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# swarmplot is also another useful plot to relate categorical vs. target variable\n",
    "# Create a Figure containing 1x3 subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12,4))\n",
    "\n",
    "sns.swarmplot(x=data.cylinders, y=data.mpg, color='#006699', ax=axes[0])\n",
    "sns.swarmplot(x=data.model_year, y=data.mpg, color='#a6cc33', ax=axes[1])\n",
    "sns.swarmplot(x=data.origin, y=data.mpg, color='#cc3399', ax=axes[2])\n",
    "\n",
    "plt.subplots_adjust(wspace=.5, hspace=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# boxplot is also another useful plot to relate categorical vs. target variable\n",
    "# Create a Figure containing 1x3 subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16,6))\n",
    "\n",
    "sns.boxplot(x=data.cylinders, y=data.mpg, color='#006699', ax=axes[0])\n",
    "sns.boxplot(x=data.model_year, y=data.mpg, color='#a6cc33', ax=axes[1])\n",
    "sns.boxplot(x=data.origin, y=data.mpg, color='#cc3399', ax=axes[2])\n",
    "\n",
    "plt.subplots_adjust(wspace=.5, hspace=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's overlay boxplot and swarmplot\n",
    "# Create a Figure containing 1x3 subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16,6))\n",
    "\n",
    "sns.boxplot(x=data.cylinders, y=data.mpg, color='#006699', ax=axes[0])\n",
    "sns.swarmplot(x=data.cylinders, y=data.mpg, color=\".5\", ax=axes[0])\n",
    "sns.boxplot(x=data.model_year, y=data.mpg, color='#a6cc33', ax=axes[1])\n",
    "sns.swarmplot(x=data.model_year, y=data.mpg, color=\".5\", ax=axes[1])\n",
    "sns.boxplot(x=data.origin, y=data.mpg, color='#cc3399', ax=axes[2])\n",
    "sns.swarmplot(x=data.origin, y=data.mpg, color=\".5\", ax=axes[2])\n",
    "\n",
    "plt.subplots_adjust(wspace=.5, hspace=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Data Preprocessing (Munging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of the Issues\n",
    "\n",
    "-  From our exploratory data analysis above, **two** main issues are observed:\n",
    "    1. The presence of **6 missing values** for the attribute <code>**horsepower**</code>\n",
    "    2. The presence of a total of **18 outliers** on the attributes <code>**horsepower**</code> and <code>**acceleration**</code>\n",
    "-  In addition to those, we should also consider how to properly handle different feature's scale as well as the fact that we are in presence of both continuous and categorical attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.1 Handling Missing Values (NA)\n",
    "\n",
    "-  There are just **6 out of 398** (i.e., approximately **1.5%**) of records containing a missing value.\n",
    "\n",
    "-  Since they do not represent a significant subset of the whole dataset, we can simply drop those records.\n",
    "\n",
    "-  Otherwise, we could mark (i.e., replace) those missing value using one of the strategies discussed (e.g., replace them with the median as computed from observed values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's go for the second option, i.e., replacing missing values on 'horsepower'\n",
    "# using the median as computed from the other observations.\n",
    "# NOTE: here's a classical example where using the mean rather than the median\n",
    "# might affect the result, as the mean is more sensitive to outliers.\n",
    "# NOTE: by default, median() does not include NAs in the computation.\n",
    "# In other words, we don't need to explicitly tell pandas to work on non-NA values:\n",
    "# data.horsepower[data.horsepower.notnull()].median()\n",
    "\n",
    "data.horsepower.fillna(data.horsepower.median(), inplace=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.2 Handling Outliers\n",
    "\n",
    "-  There are **18 outliers** shared between <code>**horsepower**</code> and <code>**acceleration**</code>.\n",
    "\n",
    "-  Like missing values, outliers can be simply discarded as well (i.e., a process which is also known as **trimming** or **truncation**).\n",
    "\n",
    "-  Another approach is called **winsorizing** and consists of replacing outliers with a specified percentile of the data (e.g., a 90% winsorization would see all data below the 5th percentile set to the 5th percentile, and data above the 95th percentile set to the 95th percentile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Python can winsorize data using 'scipy.stats' module.\n",
    "# Example:\n",
    "a = pd.Series([92, 19, 101, 58, 1053, 91, 26, 78, 10, 13, -40, 101, 86, 85, 15, 89, 89, 28, -5, 41])\n",
    "\n",
    "print(\"Length of a: {}\".format(a.shape[0]))\n",
    "print(\"Mean of a: {}\".format(a.mean()))\n",
    "print(\"Median of a: {}\".format(a.median()))\n",
    "print(\"Sorted a: {}\".format(np.sort(a)))\n",
    "q_005, q_95 = a.quantile([0.05, 0.95])\n",
    "print(\"5th percentile of a: {:.2f}\".format(q_005))\n",
    "print(\"95th percentile of a: {:.2f}\".format(q_95))\n",
    "stats.mstats.winsorize(a, limits=0.05, inplace=True)\n",
    "print(\"Sorted a: {}\".format(np.sort(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's winsorize 'horsepower' and 'acceleration'\n",
    "stats.mstats.winsorize(data.horsepower, limits=0.0375, inplace=True)\n",
    "stats.mstats.winsorize(data.acceleration, limits=0.0375, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's verify the outliers are actually gone\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,12))\n",
    "y_labeler(axes)\n",
    "sns.boxplot(data.displacement, color='#808080', ax=axes[0,0])\n",
    "sns.boxplot(data.horsepower, color='#df2020', ax=axes[0,1])\n",
    "sns.boxplot(data.weight, color='#0033cc', ax=axes[1,0])\n",
    "sns.boxplot(data.acceleration, color='#009933', ax=axes[1,1])\n",
    "plt.subplots_adjust(wspace=.3, hspace=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.3 Encoding Categorical Features\n",
    "\n",
    "-  Categorical variables are typically stored as text values which represent various traits. \n",
    "\n",
    "-  Some examples include <code>**color**</code> = {\"Red\", \"Yellow\", \"Blue\"), <code>**size**</code> =  (\"Small\", \"Medium\", \"Large\"), etc.\n",
    "\n",
    "-  Many ML algorithms can support categorical values without further manipulation but there are many others that do not. \n",
    "\n",
    "-  Therefore, the analyst is faced with the challenge of figuring out how to turn these text attributes into **numerical values** for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approach 1: Label Encoding\n",
    "\n",
    "-  Label encoding is simply converting each value in a column to a number. \n",
    "\n",
    "-  For example, the <code>**model_year**</code> column contains 13 different values. We could choose to encode it like this:\n",
    "\n",
    "```python\n",
    "70 --> 0\n",
    "71 --> 1\n",
    "72 --> 2\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approach 2: One-Hot Encoding\n",
    "\n",
    "-  Label encoding is straightforward but it has the disadvantage that numeric values can be \"misinterpreted\" by the learning algorithms. \n",
    "\n",
    "-  For example, the value of 0 is obviously less than the value of 4 but is that what we really aim for? For example, does a vehicle from 1974 have \"4x\" more weight than one from 1971?\n",
    "\n",
    "-  A common alternative approach is called **one hot encoding**. Here, the basic strategy is to convert each category value into a new column and assigns a 1 or 0 (True/False) value to the column. \n",
    "\n",
    "-  This has the benefit of not weighting a value improperly but does have the downside of adding more columns to the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# In pandas we can achieve easily one-hot encoding using the 'get_dummies()' function\n",
    "categorical_features = ['cylinders', 'model_year', 'origin']\n",
    "data_with_dummies = pd.get_dummies(data, columns = categorical_features)\n",
    "data_with_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Just as a convention, I prefer to place the column to be predicted\n",
    "# as the last one.\n",
    "columns = data_with_dummies.columns.tolist()\n",
    "# Popping out 'mpg' from the list and insert it back at the end.\n",
    "columns.insert(len(columns), columns.pop(columns.index('mpg')))\n",
    "\n",
    "# Let's refactor the DataFrame using this new column index\n",
    "data_with_dummies = data_with_dummies.loc[:, columns]\n",
    "data_with_dummies.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# The categorical variable 'car_name' contains a lot of different values.\n",
    "# Using one-hot encoding might lead to a very sparse dataset, as we need\n",
    "# to map a single column to 305 columns!\n",
    "data_with_dummies.car_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Three solutions can be designed to tackle with this issue:\n",
    "1) Just drop the column 'car_name' (i.e., our model won't rely on that feature for prediction)\n",
    "2) Use one-hot encoding scheme and deal with sparsity data (i.e., possibly leading to overfitting)\n",
    "3) Trade-off: try to build another column which somehow reduces (i.e., cluster) similar values together\n",
    "and then apply one-hot encoding.\n",
    "Let's see how to perform 3)\n",
    "\"\"\"\n",
    "# Suppose we want to create another column called 'automaker_name', which simply contains\n",
    "# the name of the automaker, disregarding the model.\n",
    "# For example, automaker_name('ford gran torino') = automaker_name('ford f250') = 'ford'\n",
    "data_with_dummies['automaker_name'] = data_with_dummies['car_name'].map(lambda x:\n",
    "                                                                       x.split(' ')[0])\n",
    "data_with_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_dummies['car_name'].map(lambda x: x.split(' ')[0]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's see how many distinct values we have now for this nvalue_countsrical variable\n",
    "\"\"\"\n",
    "data_with_dummies['automaker_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def sanitize_automaker_name(car_name):\n",
    "    s = car_name.split(' ')[0]\n",
    "    if s == 'vw' or s == 'vokswagen':\n",
    "        return car_name.replace(s,'volkswagen')\n",
    "    if s == 'chevroelt' or s == 'chevy':\n",
    "        return car_name.replace(s,'chevrolet')\n",
    "    if s == 'maxda':\n",
    "        return car_name.replace(s,'mazda')\n",
    "    if s == 'mercedes':\n",
    "        return car_name.replace(s,'mercedes-benz')\n",
    "    if s == 'toyouta':\n",
    "        return car_name.replace(s,'toyota')\n",
    "    return car_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use the 'sanitize_automaker_name' function to update 'car_name' values.\n",
    "\"\"\"\n",
    "data_with_dummies['car_name'] = data_with_dummies['car_name'].map(lambda x:\n",
    "                                                                 sanitize_automaker_name(x))\n",
    "data_with_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Re-apply the function on the sanitized version of 'car_name'.\n",
    "\"\"\"\n",
    "data_with_dummies['automaker_name'] = data_with_dummies['car_name'].map(lambda x: \n",
    "                                                                        x.split(' ')[0])\n",
    "data_with_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's see how many distinct values we have now for this new categorical variable.\n",
    "\"\"\"\n",
    "data_with_dummies['automaker_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create the set of the top-10 automakers\n",
    "top_10_automakers = set(data_with_dummies['automaker_name'].value_counts().index[:10])\n",
    "# Label with 'other' any automaker_name which is not in the list above\n",
    "data_with_dummies['automaker_name'] = np.where(data_with_dummies.\n",
    "                                               automaker_name.isin(top_10_automakers), \n",
    "                                              data_with_dummies.automaker_name,\n",
    "                                              'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's verify we did it right!\n",
    "\"\"\"\n",
    "data.loc[data_with_dummies.automaker_name == 'other', 'car_name'].map(lambda x: \n",
    "                                                                      x.split(' ')[0] not in top_10_automakers).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We now categorize (i.e., discretize) 'automaker_name' using the 10 + 1 discrete values above.\n",
    "\"\"\"\n",
    "categorical_features = ['automaker_name']\n",
    "data_with_dummies = pd.get_dummies(data_with_dummies, columns = categorical_features)\n",
    "data_with_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Just as a convention, I prefer to place the column to be predicted\n",
    "# as the last one.\n",
    "columns = data_with_dummies.columns.tolist()\n",
    "# Popping out 'mpg' from the list and insert it back at the end.\n",
    "columns.insert(len(columns), columns.pop(columns.index('mpg')))\n",
    "# Popping out 'automaker_name_other' from the list and insert it after 'automaker_name_volkswagen'.\n",
    "columns.insert(columns.index('automaker_name_volkswagen'), columns.pop(columns.index('automaker_name_other')))\n",
    "# Popping out 'car_name' from the list and insert it right before 'mpg'\n",
    "columns.insert(-1, columns.pop(columns.index('car_name')))\n",
    "# Let's refactor the DataFrame using this new column index\n",
    "data_with_dummies = data_with_dummies.loc[:, columns]\n",
    "data_with_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.4 Standardize Feature Scale\n",
    "\n",
    "-  Some learning models are sensitive to different feature scales appearing on the training dataset.\n",
    "\n",
    "-  To overcome this issue, one typically standardize (i.e., normalize) the values of each continuous feature.\n",
    "\n",
    "-  Two main strategies are usually enacted:\n",
    "    -  **min-max** normalization\n",
    "    -  **z-score** standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's deep copy our DataFrame again\n",
    "data_norm_0_1 = data_with_dummies.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The easiest way to normalize a (sub)set of features is as follows.\n",
    "\"\"\"\n",
    "# 1. Decide which list of features to standardize\n",
    "features_to_standardize = ['displacement', 'horsepower', 'weight', 'acceleration']\n",
    "\n",
    "# 2. Select those features (i.e., DataFrame columns) and apply, for example, min-max normalization\n",
    "data_norm_0_1[features_to_standardize] = ((data_norm_0_1[features_to_standardize] - \n",
    "                                          data_norm_0_1[features_to_standardize].min()) \n",
    "                                          / (data_norm_0_1[features_to_standardize].max() - \n",
    "                                             data_norm_0_1[features_to_standardize].min()))\n",
    "# 3. Verify the result\n",
    "data_norm_0_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's make another deep copy of our DataFrame\n",
    "data_std = data_with_dummies.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following three functions are used to standardize/normalize features.\n",
    "\"\"\"\n",
    "# 1. z_score computes the standard z-score of a feature value x\n",
    "def z_score(x, mu_X, sigma_X):\n",
    "    return (x - mu_X)/sigma_X\n",
    "\n",
    "# 2. min_max computes the normalized value of a feature value x in the range [-1, 1]\n",
    "def min_max(x, X_min, X_max):\n",
    "    return (2*x - X_max - X_min)/(X_max - X_min)\n",
    "\n",
    "# 3. min_max_0_1 computes the normalized value of a feature value x in the range [0, 1]\n",
    "def min_max_0_1(x, X_min, X_max):\n",
    "    return (x - X_min)/(X_max - X_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We use 'map' to call the z_score function above element-wise\n",
    "on each Series object: displacement, horsepower, weight, and acceleration.\n",
    "\"\"\"\n",
    "# 1.a. Compute the mean and std deviation of 'displacement'\n",
    "mu_displacement = data_std.displacement.mean()\n",
    "sigma_displacement = data_std.displacement.std()\n",
    "# 1.b. Call the z_score function on the 'displacement' Series\n",
    "data_std.displacement = data_std.displacement.map(lambda x: \n",
    "                                                  z_score(x, mu_displacement, \n",
    "                                                          sigma_displacement))\n",
    "\n",
    "# 2.a. Compute the mean and std deviation of 'horsepower'\n",
    "mu_horsepower = data_std.horsepower.mean()\n",
    "sigma_horsepower = data_std.horsepower.std()\n",
    "# 2.b. Call the z_score function on the 'horsepower' Series\n",
    "data_std.horsepower = data_std.horsepower.map(lambda x: \n",
    "                                              z_score(x, mu_horsepower, \n",
    "                                                      sigma_horsepower))\n",
    "\n",
    "# 3.a. Compute the mean and std deviation of 'weight'\n",
    "mu_weight = data_std.weight.mean()\n",
    "sigma_weight = data_std.weight.std()\n",
    "# 3.b. Call the z_score function on the 'weight' Series\n",
    "data_std.weight = data_std.weight.map(lambda x: \n",
    "                                      z_score(x, mu_weight, \n",
    "                                              sigma_weight))\n",
    "\n",
    "# 4.a. Compute the mean and std deviation of 'acceleration'\n",
    "mu_acceleration = data_std.acceleration.mean()\n",
    "sigma_acceleration = data_std.acceleration.std()\n",
    "# 4.b. Call the z_score function on the 'acceleration' Series\n",
    "data_std.acceleration = data_std.acceleration.map(lambda x: \n",
    "                                                  z_score(x, mu_acceleration, \n",
    "                                                          sigma_acceleration))\n",
    "\n",
    "data_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's create yet another deep copy of our DataFrame\n",
    "data_std_z = data_with_dummies.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is an even more general solution for standardizing multiple features\n",
    "in one shot, using customized \"normalizing\" functions.\n",
    "\"\"\"\n",
    "def standardized_continuous_features(dataset, feature_names, func=z_score):\n",
    "    \n",
    "    if func != z_score and func != min_max and func != min_max_0_1:\n",
    "        func = z_score\n",
    "            \n",
    "    for feature in feature_names:\n",
    "        print(\"Standardized feature \\\"{}\\\" using [{}] function\".format(feature, func.__name__))\n",
    "        if func == min_max or func == min_max_0_1:\n",
    "            feature_min = dataset[feature].min()\n",
    "            feature_max = dataset[feature].max()\n",
    "            print(\"Min. = {}\".format(feature_min))\n",
    "            print(\"Max. = {}\".format(feature_max))\n",
    "            dataset[feature] = dataset[feature].map(lambda x: func(x, feature_min, feature_max))\n",
    "        else:\n",
    "            feature_mean = dataset[feature].mean()\n",
    "            feature_std = dataset[feature].std()\n",
    "            print(\"Mean = {}\".format(feature_mean))\n",
    "            print(\"Std. Deviation = {}\".format(feature_std))\n",
    "            dataset[feature] = dataset[feature].map(lambda x: func(x, feature_mean, feature_std))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We call the function defined above on our deep copy, using the list of features\n",
    "that needs to be standardized.\n",
    "\"\"\"\n",
    "data_std_z = standardized_continuous_features(data_std_z, features_to_standardize)\n",
    "data_std_z.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's verify the two different approaches above lead to the same result.\"\n",
    "\"\"\"\n",
    "print((np.abs(data_std[features_to_standardize] - data_std_z[features_to_standardize]) < 0.0001).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Standardization and min-max scaling can be also performed using scikit-learn.\n",
    "\"\"\"\n",
    "# The following is the scikit-learn package which provides\n",
    "# various preprocessing capabilities\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Standardizing features using z-score\n",
    "std_scale = preprocessing.StandardScaler().fit(data_with_dummies[features_to_standardize])\n",
    "data_std = std_scale.transform(data_with_dummies[features_to_standardize])\n",
    "\n",
    "# Normalizing features using min-max\n",
    "minmax_scale = preprocessing.MinMaxScaler().fit(data_with_dummies[features_to_standardize])\n",
    "data_minmax = minmax_scale.transform(data_with_dummies[features_to_standardize])\n",
    "\n",
    "# NOTE: 'data_std' and 'data_minmax' are numpy's ndarray (i.e., not pandas' DataFrame) objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.5 Feature Selection\n",
    "\n",
    "-  This is a topic that would require an in-depth analysis.\n",
    "\n",
    "-  A very simple approach to select highly discriminant features is given by measuring how each feature correlated with the target, and pick the ones with the highest correlation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's compute feature correlation between each feature and our target 'mpg'.\n",
    "Pandas 'corr()' function when applied to a DataFrame returns the whole correlation matrix.\n",
    "This is a diagonal matrix having all 1's on its diagonal, whilst the entry (i,j) will contain\n",
    "the correlation coefficient between feature i and feature j (i != j).\n",
    "\"\"\"\n",
    "corr_matrix = data_with_dummies.corr()\n",
    "strong_corr = corr_matrix[(corr_matrix['mpg'].abs() > .5) &\n",
    "                         (corr_matrix['mpg'] != 1)].loc[:, 'mpg']\n",
    "\n",
    "print(\"Strongest correlated features:\\n{}\".\n",
    "      format(strong_corr.sort_values()))\n",
    "print()\n",
    "print(\"Strongest correlated features (absolute values):\\n{}\".\n",
    "      format(strong_corr.abs().sort_values(ascending=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
