{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataset is (344, 7)\n",
      "There are missing values in the dataset\n",
      "  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
      "0  Adelie  Torgersen           39.10           18.7              181.0   \n",
      "1  Adelie  Torgersen           39.50           17.4              186.0   \n",
      "2  Adelie  Torgersen           40.30           18.0              195.0   \n",
      "3  Adelie  Torgersen           44.45           17.3              197.0   \n",
      "4  Adelie  Torgersen           36.70           19.3              193.0   \n",
      "\n",
      "   body_mass_g     sex  \n",
      "0       3750.0    Male  \n",
      "1       3800.0  Female  \n",
      "2       3250.0  Female  \n",
      "3       4050.0    Male  \n",
      "4       3450.0  Female  \n",
      "       bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\n",
      "count      344.000000     344.000000         344.000000   344.000000\n",
      "mean        43.925000      17.152035         200.892442  4200.872093\n",
      "std          5.443792       1.969060          14.023826   799.696532\n",
      "min         32.100000      13.100000         172.000000  2700.000000\n",
      "25%         39.275000      15.600000         190.000000  3550.000000\n",
      "50%         44.450000      17.300000         197.000000  4050.000000\n",
      "75%         48.500000      18.700000         213.000000  4750.000000\n",
      "max         59.600000      21.500000         231.000000  6300.000000\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(sns.load_dataset(\"penguins\"))\n",
    "\n",
    "print(f\"The shape of the dataset is {df.shape}\")\n",
    "if df.isnull().any().any():\n",
    "  print(f\"There are missing values in the dataset\")\n",
    "else:\n",
    "  print(f\"There are no missing values in the dataset\")\n",
    "\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "df = df.apply(lambda x: x.fillna(x.median()) if is_numeric_dtype(x) else x.fillna(x.mode().iloc[0]))\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode categorical features with one-hot encoding and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
      "0           39.10           18.7              181.0       3750.0   \n",
      "1           39.50           17.4              186.0       3800.0   \n",
      "2           40.30           18.0              195.0       3250.0   \n",
      "3           44.45           17.3              197.0       4050.0   \n",
      "4           36.70           19.3              193.0       3450.0   \n",
      "\n",
      "   species_Adelie  species_Chinstrap  species_Gentoo  island_Biscoe  \\\n",
      "0            True              False           False          False   \n",
      "1            True              False           False          False   \n",
      "2            True              False           False          False   \n",
      "3            True              False           False          False   \n",
      "4            True              False           False          False   \n",
      "\n",
      "   island_Dream  island_Torgersen  sex  \n",
      "0         False              True    1  \n",
      "1         False              True   -1  \n",
      "2         False              True   -1  \n",
      "3         False              True    1  \n",
      "4         False              True   -1  \n"
     ]
    }
   ],
   "source": [
    "categorical_features = [col for col in df.columns if not is_numeric_dtype(df[col]) and col != \"sex\"]\n",
    "data_with_dummies = pd.get_dummies(df, columns=categorical_features)\n",
    "\n",
    "# as a convention, I prefer to place the column to be predicted as the last one\n",
    "columns = data_with_dummies.columns.tolist()\n",
    "columns.insert(len(columns), columns.pop(columns.index(\"sex\")))\n",
    "data_with_dummies = data_with_dummies.loc[:, columns]\n",
    "\n",
    "df = data_with_dummies\n",
    "\n",
    "df.sex = df.sex.map(lambda x: 1 if x == \"Male\" else -1)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the model with hold-out approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (275, 10)\n",
      "Test set shape: (69, 10)\n",
      "***** performance on the test set *****\n",
      "accuracy = 0.841\n",
      "***** classification report *****\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.87      0.79      0.83        33\n",
      "           1       0.82      0.89      0.85        36\n",
      "\n",
      "    accuracy                           0.84        69\n",
      "   macro avg       0.84      0.84      0.84        69\n",
      "weighted avg       0.84      0.84      0.84        69\n",
      "\n",
      "***** performance on the test set *****\n",
      "accuracy = 0.536\n",
      "***** classification report *****\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.03      0.06        33\n",
      "           1       0.53      1.00      0.69        36\n",
      "\n",
      "    accuracy                           0.54        69\n",
      "   macro avg       0.76      0.52      0.38        69\n",
      "weighted avg       0.75      0.54      0.39        69\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1337, stratify=y)\n",
    "\n",
    "print(f\"Train set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "def evaluate(true_values, predicted_values):\n",
    "  print(f\"accuracy = {accuracy_score(true_values, predicted_values):.3f}\")\n",
    "\n",
    "# test logistic regression\n",
    "model = LogisticRegression(solver=\"liblinear\", verbose=False)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"***** performance on the test set *****\")\n",
    "evaluate(y_test, model.predict(X_test))\n",
    "\n",
    "print(f\"***** classification report *****\")\n",
    "print(classification_report(y_test, model.predict(X_test)))\n",
    "\n",
    "# test perceptron\n",
    "model = Perceptron()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"***** performance on the test set *****\")\n",
    "evaluate(y_test, model.predict(X_test))\n",
    "\n",
    "print(f\"***** classification report *****\")\n",
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's try to use cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Evaluate Average Performance on Cross-Validation Set *****\n",
      "Avg. Test Set Accuracy = 0.802\n",
      "***** Evaluate Average Performance on Cross-Validation Set *****\n",
      "Avg. Test Set Accuracy = 0.820\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver=\"liblinear\")\n",
    "cv = cross_validate(model, X, y, cv=10, scoring=(\"roc_auc\", \"accuracy\"), return_train_score=True)\n",
    "pd.DataFrame(cv)\n",
    "\n",
    "# model evaluation using cross-validation\n",
    "print(\"***** Evaluate Average Performance on Cross-Validation Set *****\")\n",
    "print(\"Avg. Test Set Accuracy = {:.3f}\".format(np.mean(cv[\"test_accuracy\"])))\n",
    "\n",
    "model = LogisticRegression(solver = \"liblinear\")\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=1337)\n",
    "cv = cross_validate(model, X, y, cv=k_fold, scoring=(\"roc_auc\", \"accuracy\"), return_train_score=True)\n",
    "\n",
    "# model evaluation using cross-validation\n",
    "print(\"***** Evaluate Average Performance on Cross-Validation Set *****\")\n",
    "print(\"Avg. Test Set Accuracy = {:.3f}\".format(np.mean(cv[\"test_accuracy\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selection and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Evaluate Performance on Validation Set *****\n",
      "{'C': {0.01: 0.8, 0.05: 0.8, 0.1: 0.8, 0.5: 0.8, 1: 0.8, 2: 0.8}}\n",
      "***** Best Accuracy Score on Validation Set *****\n",
      "{'C': (0.01, 0.8)}\n",
      "***** Evaluate Performance on the whole Test Set *****\n",
      "accuracy = 0.812\n"
     ]
    }
   ],
   "source": [
    "models_and_hyperparams = {\n",
    "  \"LogisticRegression\": (\n",
    "    LogisticRegression(solver=\"liblinear\"),\n",
    "    {\"C\": [0.01, 0.05, 0.1, 0.5, 1, 2]},\n",
    "  )\n",
    "}\n",
    "\n",
    "\n",
    "# outer splitting: training vs test set (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  X, y, test_size=0.2, random_state=73, stratify=y\n",
    ")\n",
    "\n",
    "# inner splitting (within the outer training set): training vs validation (80/20)\n",
    "# training set is used to train the model, validation set is used to select the best hyperparameters\n",
    "X_train_train, X_validation, y_train_train, y_validation = train_test_split(\n",
    "  X_train, y_train, test_size=0.2, random_state=1337, stratify=y_train\n",
    ")\n",
    "\n",
    "training_scores = {}\n",
    "validation_scores = {}\n",
    "\n",
    "best_training_score = {}\n",
    "best_validation_score = {}\n",
    "\n",
    "model = models_and_hyperparams[\"LogisticRegression\"][0]\n",
    "hyperparams = models_and_hyperparams[\"LogisticRegression\"][1]\n",
    "\n",
    "for hp in hyperparams:\n",
    "  training_scores[hp] = {}\n",
    "  validation_scores[hp] = {}\n",
    "  \n",
    "  for val in hyperparams[hp]:\n",
    "    model.set_params(**{hp: val})\n",
    "    \n",
    "    model.fit(X_train_train, y_train_train)\n",
    "    \n",
    "    training_score = accuracy_score(y_train_train, model.predict(X_train_train))\n",
    "    training_scores[hp][val] = training_score\n",
    "    \n",
    "    validation_score = accuracy_score(y_validation, model.predict(X_validation))\n",
    "    validation_scores[hp][val] = validation_score\n",
    "    \n",
    "    if not best_validation_score:\n",
    "      best_validation_score[hp] = (val, validation_score)\n",
    "    else:\n",
    "      if best_validation_score[hp][1] < validation_score:\n",
    "        best_validation_score[hp] = (val, validation_score)\n",
    "\n",
    "print(\"***** Evaluate Performance on Validation Set *****\")\n",
    "print(validation_scores)\n",
    "print(\"***** Best Accuracy Score on Validation Set *****\")\n",
    "print(best_validation_score)\n",
    "\n",
    "# we set the model's hyperparameters to those leading to the best score on the validation test\n",
    "best_params = dict([(list(best_validation_score.keys())[0], list(best_validation_score.values())[0][0])])\n",
    "model.set_params(**best_params)\n",
    "\n",
    "# we fit this model to the whole training set portion\n",
    "model.fit(X_train, y_train)\n",
    "print(\"***** Evaluate Performance on the whole Test Set *****\")\n",
    "evaluate(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   0         1         2         3         4  \\\n",
      "RandomForestClassifier      0.892857  0.892857  0.857143  0.785714  0.785714   \n",
      "GradientBoostingClassifier  0.785714  0.928571  0.857143  0.750000  0.750000   \n",
      "MLP                         0.535714  0.464286  0.535714  0.607143  0.500000   \n",
      "\n",
      "                                   5         6         7         8         9  \\\n",
      "RandomForestClassifier      0.925926  0.925926  0.925926  0.851852  0.888889   \n",
      "GradientBoostingClassifier  0.962963  0.925926  0.888889  0.851852  0.777778   \n",
      "MLP                         0.518519  0.555556  0.666667  0.592593  0.851852   \n",
      "\n",
      "                              avg_cv    std_cv  \n",
      "RandomForestClassifier      0.873280  0.048034  \n",
      "GradientBoostingClassifier  0.847884  0.071076  \n",
      "MLP                         0.582804  0.100146  \n",
      "***** Evaluate Performance on Training Set *****\n",
      "accuracy = 1.000\n",
      "***** Evaluate Performance on Test Set *****\n",
      "accuracy = 0.957\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  X, y, test_size=0.2, random_state=1337, stratify=y\n",
    ")\n",
    "\n",
    "models = {\n",
    "  # \"LogisticRegression\": LogisticRegression(solver=\"liblinear\", max_iter=1000),\n",
    "  # \"LinearSVC\": LinearSVC(),\n",
    "  # \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "  \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "  \"GradientBoostingClassifier\": GradientBoostingClassifier(),\n",
    "  # \"Knn\": KNeighborsClassifier(),\n",
    "  \"MLP\": MLPClassifier()\n",
    "  # i could add more models here\n",
    "}\n",
    "\n",
    "k_fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=1337)\n",
    "cv_scores = {}\n",
    "for model_name, model in models.items():\n",
    "  cv_scores[model_name] = cross_val_score(model, X_train, y_train, cv=k_fold, scoring=\"accuracy\")\n",
    "\n",
    "cv_df = pd.DataFrame(cv_scores).transpose()\n",
    "\n",
    "cv_df['avg_cv'] = np.mean(cv_df, axis=1)\n",
    "cv_df['std_cv'] = np.std(cv_df, axis=1)\n",
    "cv_df = cv_df.sort_values(['avg_cv', 'std_cv'], ascending=[False,True])\n",
    "\n",
    "print(cv_df.head())\n",
    "\n",
    "# model Selection: Logistic Regression is the best overall method, therefore we pick that!\n",
    "# now we need to provide an estimate of its generalization performance. \n",
    "# to do so, we evaluate it against the test set portion we previously held out.\n",
    "model = models[cv_df.index[0]]\n",
    "# re-fit the best selected model on the whole training set\n",
    "model.fit(X_train, y_train)\n",
    "# evaluation\n",
    "print(\"***** Evaluate Performance on Training Set *****\")\n",
    "evaluate(y_train, model.predict(X_train))\n",
    "print(\"***** Evaluate Performance on Test Set *****\")\n",
    "evaluate(y_test, model.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
