{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fundamentals of Information Systems\n",
    "\n",
    "## Python Programming (for Data Science)\n",
    "\n",
    "### Master's Degree in Data Science\n",
    "\n",
    "#### Giorgio Maria Di Nunzio\n",
    "#### (Courtesy of Gabriele Tolomei FIS 2018-2019)\n",
    "<a href=\"mailto:giorgiomaria.dinunzio@unipd.it\">giorgiomaria.dinunzio@unipd.it</a><br/>\n",
    "University of Padua, Italy<br/>\n",
    "2021/2022<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 12: The Regression Problem - Example (Part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Import stats module from scipy, which contains a large number \n",
    "# of probability distributions as well as an exhaustive library of statistical functions.\n",
    "import scipy.stats as stats\n",
    "# Import the sub-module 'linear_model' of scikit-learn\n",
    "# this sub-module implements generalized linear models.\n",
    "# API: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model\n",
    "from sklearn import linear_model\n",
    "# Import two quality metrics which will be used to assess the quality\n",
    "# of our predictions: MSE and R squared (a.k.a. coefficient of determination)\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary of Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Path to the local dataset file\n",
    "DATASET_PATH = \"./data/auto-mpg-regression/dataset.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', \n",
    "           'acceleration', 'model_year', 'origin', 'car_name']\n",
    "\n",
    "data = pd.read_csv(DATASET_PATH, sep=\"\\t\", header=None, \n",
    "                   names=columns,\n",
    "                  na_values={'horsepower':'?'})\n",
    "\n",
    "print(\"Shape of the dataset: {}\".format(data.shape))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Change the Column Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Just as a convention, I prefer to place the column to be predicted\n",
    "# as the last one.\n",
    "columns = data.columns.tolist()\n",
    "# Popping out 'mpg' from the list and insert it back at the end.\n",
    "columns.insert(len(columns), columns.pop(columns.index('mpg')))\n",
    "# Let's refactor the DataFrame using this new column index\n",
    "data = data.loc[:, columns]\n",
    "data.head()\n",
    "# Alternatively to 'loc' we can also use 'reindex()'\n",
    "# data = data.reindex(columns=columns)\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Apply Some Simple Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Suppose we want to convert displacement unit from cubic inch to litre\n",
    "# There is a useful conversion table which tells us how to do that.\n",
    "# 1 cubic inch = 0.016387064 litre\n",
    "CI_TO_LITRE = 0.016387064\n",
    "data.displacement *= CI_TO_LITRE\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Handling Missing Values (NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's replace missing values on 'horsepower'\n",
    "# using the median as computed from the other observations.\n",
    "# NOTE: here's a classical example where using the mean rather than the median\n",
    "# might affect the result, as the mean is more sensitive to outliers.\n",
    "# NOTE: by default, median() does not include NAs in the computation.\n",
    "# In other words, we don't need to explicitly tell pandas to work on non-NA values:\n",
    "# data.horsepower[data.horsepower.notnull()].median()\n",
    "data.horsepower.fillna(data.horsepower.median(), inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's winsorize 'horsepower' and 'acceleration'\n",
    "_ = stats.mstats.winsorize(data.horsepower, limits=0.0375, inplace=True)\n",
    "_ = stats.mstats.winsorize(data.acceleration, limits=0.0375, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoding Categorical Features: One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# In pandas we can achieve easily one-hot encoding using the 'get_dummies()' function\n",
    "categorical_features = ['cylinders', 'model_year', 'origin']\n",
    "data = pd.get_dummies(data, columns = categorical_features)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Just as a convention, I prefer to place the column to be predicted\n",
    "# as the last one.\n",
    "columns = data.columns.tolist()\n",
    "# Popping out 'mpg' from the list and insert it back at the end.\n",
    "columns.insert(len(columns), columns.pop(columns.index('mpg')))\n",
    "# Let's refactor the DataFrame using this new column index\n",
    "data = data.loc[:, columns]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def sanitize_automaker_name(car_name):\n",
    "    s = car_name.split(' ')[0]\n",
    "    if s == 'vw' or s == 'vokswagen':\n",
    "        return car_name.replace(s,'volkswagen')\n",
    "    if s == 'chevroelt' or s == 'chevy':\n",
    "        return car_name.replace(s,'chevrolet')\n",
    "    if s == 'maxda':\n",
    "        return car_name.replace(s,'mazda')\n",
    "    if s == 'mercedes':\n",
    "        return car_name.replace(s,'mercedes-benz')\n",
    "    if s == 'toyouta':\n",
    "        return car_name.replace(s,'toyota')\n",
    "    return car_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's sanitize automaker name on 'car_name' column\n",
    "\"\"\"\n",
    "data['car_name'] = data['car_name'].map(lambda x: sanitize_automaker_name(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Three solutions can be designed to tackle with this issue:\n",
    "1) Just drop the column 'car_name' (i.e., our model won't rely on that feature for prediction)\n",
    "2) Use one-hot encoding scheme and deal with sparsity data (i.e., possibly leading to overfitting)\n",
    "3) Trade-off: try to build another column which somehow reduces (i.e., cluster) similar values together\n",
    "and then apply one-hot encoding.\n",
    "Let's see how to perform 3)\n",
    "\"\"\"\n",
    "# Suppose we want to create another column called 'automaker_name', which simply contains\n",
    "# the name of the automaker, disregarding the model.\n",
    "# For example, automaker_name('ford gran torino') = automaker_name('ford f250') = 'ford'\n",
    "data['automaker_name'] = data['car_name'].map(lambda x: x.split(' ')[0])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create the set of the top-10 automakers\n",
    "top_10_automakers = set(data['automaker_name'].value_counts()[:10].index)\n",
    "# Label with 'other' any automaker_name which is not in the list above\n",
    "data['automaker_name'] = np.where(data['automaker_name'].isin(top_10_automakers), \n",
    "                                               data['automaker_name'],\n",
    "                                              'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "categorical_features = ['automaker_name']\n",
    "data = pd.get_dummies(data, columns = categorical_features)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Just as a convention, I prefer to place the column to be predicted\n",
    "# as the last one.\n",
    "columns = data.columns.tolist()\n",
    "# Popping out 'mpg' from the list and insert it back at the end.\n",
    "columns.insert(len(columns), columns.pop(columns.index('mpg')))\n",
    "# Popping out 'automaker_name_other' from the list and insert it after 'automaker_name_volkswagen'.\n",
    "columns.insert(columns.index('automaker_name_volkswagen'), columns.pop(columns.index('automaker_name_other')))\n",
    "# Popping out 'car_name' from the list and insert it right before 'mpg'\n",
    "columns.insert(-1, columns.pop(columns.index('car_name')))\n",
    "# Let's refactor the DataFrame using this new column index\n",
    "data = data.loc[:, columns]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. Building a Predictive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4.1 Dataset Splitting: _Training_ vs. _Test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the feature matrix from our original DataFrame.\n",
    "\"\"\"\n",
    "# Feature matrix X is composed of all the columns \n",
    "# except 'car_name' and 'mpg' (i.e., the target)\n",
    "X = data.iloc[:,:-2]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Similarly, we want to extract the target column vector y.\n",
    "\"\"\"\n",
    "y = data.mpg\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's split our dataset with scikit-learn 'train_test_split' function, \n",
    "which splits the input dataset into training and test set, respectively.\n",
    "We want the training set to account for 80% of the original dataset, whilst \n",
    "the test set to account for the remaining 20%.\n",
    "Additionally, we would like to take advantage of stratified sampling,\n",
    "so as to obtain the same target distribution in both the training and the test sets.\n",
    "PROBLEM: When target (y) is categorical this is straightforward (just use 'stratify=y').\n",
    "If y is continous, instead, we need to 'discretize' it before use stratified sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Check the number of observations contained in 'y'.\n",
    "print(\"N. of observations = {}\".format(y.shape[0]))\n",
    "\n",
    "# 2. Suppose we want to use the Friedman-Diaconis rule to devise the number of bins.\n",
    "# bin_width = 2 * IQR/n^{1/3}; n_bins = (max - min)/bin_width\n",
    "y_q1, y_q3 = y.quantile([.25, .75])\n",
    "print(\"1st quartile = {:.2f}\".format(y_q1))\n",
    "print(\"3rd quartile = {:.2f}\".format(y_q3))\n",
    "y_IQR = (y_q3 - y_q1)\n",
    "bin_width = 2 * y_IQR/math.pow(y.shape[0], 1/3)\n",
    "n_bins = int((y.max() - y.min())//bin_width)\n",
    "print(\"N. of bins = {}\".format(n_bins))\n",
    "\n",
    "# 3. Save our y values in a new ndarray,\n",
    "# broken down by the bins created above.\n",
    "\n",
    "#bins = np.linspace(0, y.shape[0], n_bins)\n",
    "\n",
    "bins = np.linspace(y.min(), y.max(), n_bins)\n",
    "\n",
    "y_binned = np.digitize(y, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_binned, return_counts=True)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 4. Pass y_binned to the stratify argument,\n",
    "# and sklearn will handle the rest\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y_binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(y.min(), y.max(), n_bins)\n",
    "\n",
    "y_binned = np.digitize(y, bins[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_binned, return_counts=True)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 4. Pass y_binned to the stratify argument,\n",
    "# and sklearn will handle the rest\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y_binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Training Set shape: {}\".format(X_train.shape))\n",
    "print(\"Test Set shape: {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Scaling: Why/When\n",
    "\n",
    "-  <span style=\"color: red\">**REMEMBER:**</span> Not every learning models are sensitive to different feature scales! \n",
    "\n",
    "-  For example, in the case of Linear Regression the vector of model parameters we come up with when we minimize the MSE - using either the pseudo-inverse (closed-form) or gradient descent (iterative) solution - is **not** affected by different feature scales, except for a constant.\n",
    "\n",
    "-  You can convince yourself of this by computing the gradient of MSE using non-scaled and scaled features.\n",
    "\n",
    "-  Other models, instead, are not invariant with respect to scalar transformations of the input (features), and leads to completely different results if features are not properly scaled. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Scaling: How\n",
    "\n",
    "-  Feature scaling **cannot** be done looking at the whole dataset!\n",
    "\n",
    "-  In other words, either you standardize (using **z-scores**) or normalize (using **min-max**) your features you **must** do it considering only the training set portion of your dataset.\n",
    "\n",
    "-  The same scaling, then, should be applied to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's use two different feature scaling strategies: standard z-scores and min-max\n",
    "\"\"\"\n",
    "# The following is the scikit-learn package which provides\n",
    "# various preprocessing capabilities\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Standardizing features using z-score\n",
    "std_scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_std = std_scaler.transform(X_train)\n",
    "# Alternatively, using pure pandas:\n",
    "# X_train_mean = X_train.mean()\n",
    "# X_train_std = X_train.std()\n",
    "# X_train_std = (X_train - X_train_mean)/X_train_std\n",
    "\n",
    "# Normalizing features using min-max\n",
    "minmax_scaler = preprocessing.MinMaxScaler().fit(X_train)\n",
    "X_train_minmax = minmax_scaler.transform(X_train)\n",
    "# Alternatively, using pure pandas:\n",
    "# X_train_max = X_train.max()\n",
    "# X_train_min = X_train.min()\n",
    "# X_train_minmax = (X_train - X_train_min)/(X_train_max - X_train_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "At this stage, we can work with 3 different feature matrices:\n",
    "- The original one: X_train\n",
    "- The standardized one: X_train_std\n",
    "- The min-max normalized one: X_train_minmax\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "General function used to fit a given model to a training set (X_train, y_train)\n",
    "\"\"\"\n",
    "def fit(model, X_train, y_train):\n",
    "    model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "General function used to make predictions on the test set (X_test)\n",
    "\"\"\"\n",
    "def predictions(model, X_test):\n",
    "    return model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "General function used to assess the quality of predictions\n",
    "in terms of two scores: MSE and R2\n",
    "\"\"\"\n",
    "def evaluate(true_values, predicted_values):\n",
    "    # The mean squared error\n",
    "    print(\"Mean Squared Error (MSE) = {:.6f}\".\n",
    "          format(mean_squared_error(true_values, predicted_values)))\n",
    "    \n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print(\"Coefficient of Determination (R2 score) = {:.6f}\".\n",
    "          format(r2_score(true_values, predicted_values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def print_regression_coefficients(model, columns):\n",
    "    \n",
    "    print(\"Regression Coefficients:\\n{}\".format(\"\\n\".join([\"[\"\n",
    "                                                           + columns[i].strip() \n",
    "                                                           + \"={:.2f}\".\n",
    "                                                           format(model.coef_[i]) \n",
    "                                                           +\"]\" \n",
    "                                                           for i in range(0, len(columns))]\n",
    "                                                         )\n",
    "                                               )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_true_vs_predicted(y_true, y_predicted):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(y_true, y_predicted, edgecolors=(0, 0, 0), color='red')\n",
    "    ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--', lw=4)\n",
    "    ax.set_xlabel('True MPG')\n",
    "    ax.set_ylabel('Predicted MPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "model = linear_model.LinearRegression()\n",
    "\n",
    "# 1. Try to fit this linear regressor to our original training set\n",
    "fit(model, X_train, y_train)\n",
    "\n",
    "# 2. Assess the quality of predictions made on the same training set\n",
    "print(\"***** Evaluate Predictions on Training Set *****\")\n",
    "\n",
    "mpg_y_train_pred = model.predict(X_train)\n",
    "evaluate(y_train, mpg_y_train_pred)\n",
    "plot_true_vs_predicted(y_train, mpg_y_train_pred)\n",
    "print()\n",
    "\n",
    "# 3. Assess the quality of predictions made on the test set\n",
    "print(\"***** Evaluate Predictions on Test Set *****\")\n",
    "mpg_y_test_pred = model.predict(X_test)\n",
    "evaluate(y_test, mpg_y_test_pred)\n",
    "plot_true_vs_predicted(y_test, mpg_y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create another linear regression object\n",
    "model_std = linear_model.LinearRegression()\n",
    "\n",
    "# 1. Try to fit this linear regressor to our original training set\n",
    "fit(model_std, X_train_std, y_train)\n",
    "# 2. Scale the test set (using the same scaler fit to the training set)\n",
    "X_test_std = std_scaler.transform(X_test)\n",
    "#X_test_std = (X_test - X_train_mean)/X_train_std\n",
    "# 3. Make predictions using the testing set\n",
    "mpg_y_pred_std = model_std.predict(X_test_std)\n",
    "# 4. Evaluate predictions\n",
    "evaluate(y_test, mpg_y_pred_std)\n",
    "# 5. Plot true vs. predicted values of 'mpg'\n",
    "plot_true_vs_predicted(y_test, mpg_y_pred_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create another linear regression object\n",
    "model_minmax = linear_model.LinearRegression()\n",
    "\n",
    "# 1. Try to fit this linear regressor to our original training set\n",
    "fit(model_minmax, X_train_minmax, y_train)\n",
    "# 2. Scale the test set (using the same scaler fit to the training set)\n",
    "X_test_minmax = minmax_scaler.transform(X_test)\n",
    "#X_test_minmax = (X_test - X_train_min)/X_train_max\n",
    "# 3. Make predictions using the testing set\n",
    "mpg_y_pred_minmax = model_minmax.predict(X_test_minmax)\n",
    "# 4. Evaluate predictions\n",
    "evaluate(y_test, mpg_y_pred_minmax)\n",
    "# 5. Plot true vs. predicted values of 'mpg'\n",
    "plot_true_vs_predicted(y_test, mpg_y_pred_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $k$-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Instead of splitting training and test set, we can use k-fold cross validation.\n",
    "NOTE: Since there is no need for hyperparameter tuning here,\n",
    "we can simply run k-fold cross validation on the whole dataset (X)\n",
    "'cross_val_score' is a function which gives us the cv scores\n",
    "(R2 is the default score for linear regression).\n",
    "\"\"\"\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Reset the linear regression object\n",
    "cv_model = linear_model.LinearRegression()\n",
    "\n",
    "# Run 'cross_val_score' to get the R2 score on each fold\n",
    "# NOTE: cross validation is performed on the whole dataset as there is no\n",
    "# need for hyperparameter tuning at this stage, nor model comparison.\n",
    "cv_scores = cross_val_score(cv_model, X, y, cv=10) \n",
    "                            #scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "print(\"***** Cross Validation Summary *****\")\n",
    "for v in range(0, len(cv_scores)):\n",
    "    print(\"R2 score on fold-{} = [{}]\".format(v+1, cv_scores[v]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "sklearn.metrics.get_scorer_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Average 10-fold cross validation R2 score: {:.2f} (+/- {:.2f})\"\n",
    "      .format(cv_scores.mean(), cv_scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "By default, no shuffling is performed before data splitting.\n",
    "We can override this behavior using a specific instance of KFold class.\n",
    "\"\"\"\n",
    "from sklearn.model_selection import KFold\n",
    "# Reset the linear regression object\n",
    "cv_model = linear_model.LinearRegression()\n",
    "\n",
    "# Run 'cross_val_score' to get the R2 score on each fold\n",
    "cv_scores = cross_val_score(cv_model, X, y, cv=KFold(n_splits=10, \n",
    "                                                     shuffle=True, random_state=42))\n",
    "\n",
    "print(\"***** Cross Validation Summary *****\")\n",
    "for v in range(0, len(cv_scores)):\n",
    "    print(\"R2 score on fold-{} = [{}]\".format(v+1, cv_scores[v]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Average 10-fold cross validation R2 score (with random shuffling): {:.2f} (+/- {:.2f})\"\n",
    "      .format(cv_scores.mean(), cv_scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Instead of splitting training and test set, we can use k-fold cross validation.\n",
    "NOTE: Since there is no need for hyperparameter tuning here,\n",
    "we can simply run k-fold cross validation on the whole dataset (X)\n",
    "'cross_validate' is a function which gives us a dictionary of scores.\n",
    "\"\"\"\n",
    "from sklearn.model_selection import cross_validate\n",
    "# Reset the linear regression object\n",
    "cv_model = linear_model.LinearRegression()\n",
    "\n",
    "scoring = ['r2']\n",
    "cv_scores = cross_validate(cv_model, X, y, scoring=scoring, \n",
    "                           cv=KFold(n_splits=10, shuffle=True, random_state=42), \n",
    "                           return_train_score=True)\n",
    "print(\"***** Cross Validation Summary *****\")\n",
    "for k in cv_scores:\n",
    "    print(\"{} = [{}]\".format(k, \", \".join([\"{:.5f}\".format(v) for v in cv_scores[k]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use the outcome of k-fold cross validation to generate predictions, \n",
    "calling 'cross_val_predict' function.\n",
    "\"\"\"\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Reset the linear regression object\n",
    "cv_model = linear_model.LinearRegression()\n",
    "# We generate predictions on the basis of the result of 10-fold cv\n",
    "mpg_y_pred_cv = cross_val_predict(cv_model, X, y, cv=KFold(n_splits=10, \n",
    "                                                           shuffle=True, random_state=42))\n",
    "# Evaluate predictions\n",
    "evaluate(y, mpg_y_pred_cv)\n",
    "# Plot true vs. predicted values (using 10-fold cross validation)\n",
    "plot_true_vs_predicted(y, mpg_y_pred_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Leave-One-Out cross validation (i.e., k = n, number of observations)\n",
    "\"\"\"\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Reset the linear regression object\n",
    "cv_model = linear_model.LinearRegression()\n",
    "# We generate predictions on the basis of the result of leave-one-out\n",
    "mpg_y_pred_cv = cross_val_predict(cv_model, X, y, cv=X.shape[0])\n",
    "# Evaluate predictions\n",
    "evaluate(y, mpg_y_pred_cv)\n",
    "# Plot true vs. predicted values (using leave-one-out cross validation)\n",
    "plot_true_vs_predicted(y, mpg_y_pred_cv)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
